---
layout: post
title: " attention을 이해해보자(1)-Seq2Seq model"
comment: true

---


---
<index>

## 1. seq2seq model
2. bidirectional RNN
3. attention model
4. transformer (논문: Attention is all you need(2017))
5. transformer examples: BERT, openAI-GPT 
[the liiustrated transformer](http://jalammar.github.io/illustrated-transformer/)
---

이 문서의 전반적인 코드는 아래 tae hwan jung 님의 깃허브를 참고하였습니다. 정말 이해가 잘 되게 설명해놓은 자료이네요..
[seq2seq_tensorflow_code_by.Tae hwan jung](https://github.com/graykode/nlp-tutorial)



일단 코드를 한번 뜯어보고 난 뒤 자세한 설명을 하겠습니다.

* packages load

```
import tensorflow as tf
import numpy as np
# 텐서 자료형이나 연산자를 연결하면 모드 그래프에 저장됨.
# 현재 기본 그래프에 대한 정보를 얻는다. 
tf.get_default_graph() # none 
```

* input data sequence 

```
# input,output단어를 구성하는 글자들을 리스트에 저장
char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']
# 글자 하나하나를 index 0~ 매칭시켜 딕셔너리로 저장 
num_dic = {n: i for i, n in enumerate(char_arr)}
# 앞에 단어는 input, 뒤에 단어는 output단어
seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]

```

* seq2seq parameter

```
n_step = 5
n_hidden = 128
n_class = len(num_dic) # number of class(=number of vocab)
```

*  batch data를 생성하는 funciton

seq_data(list)를 인풋으로 받는다.

리턴값으로는 각각 리스트자료형의 input_batch, output_batch, target_batch이다.


```
def make_batch(seq_data):
    input_batch, output_batch, target_batch = [], [], []

    for seq in seq_data:
        for i in range(2):
            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))

        input = [num_dic[n] for n in seq[0]]
        output = [num_dic[n] for n in ('S' + seq[1])]
        target = [num_dic[n] for n in (seq[1] + 'E')]

        input_batch.append(np.eye(n_class)[input])
        output_batch.append(np.eye(n_class)[output])

        target_batch.append(target)

    return input_batch, output_batch, target_batch

```

```
input: input단어를 구성하는 vocab들에 해당하는 index숫자를 리스트에 저장한다.
output: output단어를 구성하는 vocab들에 해당하는 index숫자를 리스트에 저장한다., 단, 각 단어의 가장 앞에 'S'를 붙임.
target: output단어의 가장 뒤에 E를 붙임. 그렇게 새로 생겨난 단어의 인덱스를 target 리스트에 저장 

input_batch.append(np.eye(n_class)[input])

이부분을 해석해봅시다. input_batch의 리스트에 np.eye(29)[input] 을 추가한다는 것인데, np.eye(29)는 29 by 29의 identical matrix를 만들어냅니다. 그리고 29라는 숫자는 len(num_dic)인데 영어vocab의 총 수(+26개 알파벳)+S+E+P(+3) 를 말합니다. 그리고 옆에 [input]이 있으니 identical matrix를 생성하는 것 대신 행렬의 첫번째 row에서의 1값은 input list의 첫번째 index에 해당하는 숫자의 위치에 오게 됩니다. 
따라서 input length가 n_class보다 작다면 정방행렬이 아니게 되는 거죠. 따라서 input_batch는 리스트인데, 리스트 안에 각각 vocab의 one-hot vector가 저장되어지게 됩니다. one-hot vector의 차원은 (1,num_class) 가 됩니다. 

```

* making model

enc_input,dec_input,targets 모두 batch_size, max_len을 none으로 준 이유는 우리가 새로운 단어를 집어넣어서 한번 시도해보라는 글쓴이의 의도입니다. 

[layers.dense에 3차원 tensor의 연산에 관련한 자료](https://neurowhai.tistory.com/112?category=605395)

```

enc_input = tf.placeholder(tf.float32, [None, None, n_class]) # [batch_size, max_len(=encoder_step), n_class]
dec_input = tf.placeholder(tf.float32, [None, None, n_class]) # [batch_size, max_len+1(=decoder_step) (becase of 'S' or 'E'), n_class]
targets = tf.placeholder(tf.int64, [None, None]) # [batch_size, max_len+1], not one-hot

with tf.variable_scope('encode'):
    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)
    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)
    _, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input, dtype=tf.float32)
    # encoder state will go to decoder initial_state, enc_states : [batch_size, n_hidden(=128)]

with tf.variable_scope('decode'):
    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)
    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)
    outputs, _ = tf.nn.dynamic_rnn(dec_cell, dec_input, initial_state=enc_states, dtype=tf.float32)
    # outputs : [batch_size, max_len+1, n_hidden(=128)]

model = tf.layers.dense(outputs, n_class, activation=None) # model : [batch_size, max_len+1, n_class]

cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=targets))
optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)

```

* train

```
sess = tf.Session()
sess.run(tf.global_variables_initializer())
input_batch, output_batch, target_batch = make_batch(seq_data) # batch_Data 생성 

for epoch in range(5000):
    _, loss = sess.run([optimizer, cost], feed_dict={enc_input: input_batch, dec_input: output_batch, targets: target_batch})
    if (epoch + 1)%1000 == 0:
        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))
```

* test

```
def translate(word): # word는 해석할 단어 
    
    seq_data = [word, 'P' * len(word)] 

    input_batch, output_batch, _ = make_batch([seq_data])
    prediction = tf.argmax(model, 2)

    result = sess.run(prediction, feed_dict={enc_input: input_batch, dec_input: output_batch})

    decoded = [char_arr[i] for i in result[0]]
    end = decoded.index('E')
    translated = ''.join(decoded[:end])

    return translated.replace('P','')

print('test')
print('man ->', translate('man'))
print('mans ->', translate('mans'))
print('king ->', translate('king'))
print('black ->', translate('black'))
print('upp ->', translate('upp'))

```





