---
layout: post
title: "Word2Vec 알고리즘과 CBOW, SKIP-GRAM"
date: 2019-09-07
categories: NLP
---

### Word2Vec 알고리즘_수정

위 문서는 다음 사이트를 참고하여, 직접 코드를 구현하고 수식을 증명하면서 덧붙힌 설명글입니다.

1. goal: **단어** 를 **벡터** 로 바꾸는 알고리즘 => 즉, 단어의 의미를 최대한 담는 벡터를 만들려는 알고리즘을 `word embedding model`이라고 부른다.


2. 유용성: sparse vector( one-hot encoding vector) -> dense vector(distributed representation of word embedding vector) : sparse vector 형식은 한 단어가 가질 수 있는 다양한 속성들을 모두 각각의 독립적인 차원으로 표현한다. 즉, 우리가 가지고있는 단어가 N개라면, N개의 만들어진 벡터는 모두 독립이다.                                                                                                           그러나, 강아지와 고양이/ 엄마와 아빠/ 여왕과 왕 과 같은 단어들은 서로 독립이라고 할 수 없으며, 따라서 위 방식의 벡터는 단어 간 의미와 관계를 제대로 반영하지 못한다라고 할 수 있다. 그러나 dense vector의 방식으로 만들면, 단어를 벡터로 바꿀 때, 단어 간 의미와 상호관계성을 반영하여 벡터를 만듦으로, __단어의 의미가 벡터에 잘 담긴다.__ 다시말하면, 우리가 어떤 단어를 m차원 벡터로 만들겠다(=각 단어들의 feature 수를 m개라 하겠다)고 정하면, 그 속성을 m차원 벡터에 대응시키는 방식이다. 이 대응을 `embedding`이라 하며, 임베딩하는 방식은 머신러닝,딥러닝을 통해 학습된다. 


3. 임베딩 원리: 원래 N차원이여야할 벡터를 m차원으로 보통 줄이므로, (일종의 dimension reduction.) feature은 서로 버무려 output을 낸다. 따라서 임베딩벡터를 해석하는 것은 어렵다. 다만, 강아지벡터와 고양이벡터 간 유사성은 두 벡터 간 내적의 값이 클수록 유사하다고 결론내릴 수 있다.

### idea of Word2Vec

주요 알고리즘: Word2Vec 알고리즘은 일종의 비지도학습처럼 학습한다. 그러나 사실 비지도학습 알고리즘이다. 어떤 단어와 어떤 단어가 비슷하다 라고 우리가 안알려줘도, 이 알고리즘은 비슷한 단어들을 찾아낼 수 있다.

    - CBOW(continuous bag of words)model: 주변단어(맥락, context)로 target word를 예측하는 문제. 주변단어는 주로 타겟단어 주위의 몇몇단어를 말한다. 이 주변단어의 범위를 `window` 라 부르며 이것은 hyperparameter이다. sliding window를 하면서 타겟단어를 계속바꾸는 방식으로 데이터셋을 만든다. 즉 만들어진 window 하나하나가 모두 데이터셋이다. CBOW는 맥락으로 단어를 예측하는 문제를 푼다. input은 주변단어이며, one-hot encoding된 벡터 한 개이다. (1*Vobab_size) %*% (Vocab_size*want_dimension) %*%(want_dimension*Vocab_size)= 1*Vocab_size = output=target word
    
     (1*Vobab_size,onehot encoding, k번째원소만 1 나머지는 0) %*% (Vocab_size*want_dimension) = t(W)*x= **embedding된 벡터.** => (Vocab_size*want_dimension)=W matrix의 k번째 행만 남는다. 
     
     이제 히든레이어에서 출력레이어로 넘어가기 위해 또다른 가중치행렬 W'가 필요하다. W'는 (want_dimension*Vocab_size)크기의 행렬이다. 이 행렬을 이용하여 **우리가 가지고 있는 모든단어에 대한 output score를 계산할 수 있다.** 
     
     이렇게 예측된 점수를 각 단어의 확률값으로 바꾸기 위해 softmax함수를 쓴다. 0~1 사이의 값으로 바꿔준다는 이야기. 
     
     
    
    - skip-gram model: specific word in the middle of a sentence 즉 input word가 주어지면, nearby 단어들을 보고 하나를 랜덤으로 뽑는다. 여기서 nearby word란, input단어 주변의 window size내의 하나의 단어를 말한다. output 확률은 다음과 관련있다. input으로 '소련'이 오면 output이 'union'이나 '러시아'인 단어가 가지는 확률이 '수박'이나 '캥커루'가 가지는 확률보다 클 것이다. 즉, 인풋단어와 가장 연관있는 단어를 output으로 내보내는 것. 
    
    모델의 train은 다음과 같다. word pairs를 feeding해준다.예를 들면, 한 문장이 있다. " the quick brown fox jumps over the lazy dog" 가 있다면, window size=2라 했을 때, 첫 단어부터 끝단어까지 한번씩 input단어가 되며, 그에 해당하는 pair단어는 window size내에 있는 단어들이 한번씩 input단어의 페어로 매칭이 된다.   네트워크는 각 페어쌍이 나타나는 횟수를 학습한다. 예를 들면, 학습데이터는 (소련,러시아)쌍을 (소련, 수박) 쌍보다 많이 가지고 있을 것이다. 
    
    모델에 대한 디테일: cbow의 히든메트릭스와 softmax 분류기를 사용한다. 이때 input vector은 원핫인코딩 된 어떤 단어의 벡터이며, 중간에 W메트릭스를 만나서 embedding vector로 변환시켜지며, 그 변환된 벡터가 다시 softmax를 거치면서 단어장에 있는 모든 단어에 대한 확률이 output으로 벡터형식으로 도출된다. 이때의 확률이 의미하는 바는 `input 단어와 가장 nearby있는 단어`이다. 
